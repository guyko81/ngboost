{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beta-Bernoulli Classification with NGBoost\n",
    "\n",
    "**Problem**: Binary classification with calibrated uncertainty. Unlike standard\n",
    "classifiers that output a single probability, the Beta-Bernoulli gives you a\n",
    "full *distribution over the probability itself* via its Beta(alpha, beta) prior.\n",
    "\n",
    "**When to use**:\n",
    "- Medical diagnosis: not just \"80% chance of disease\" but \"somewhere between 60-95%\"\n",
    "- Fraud detection: flag transactions where the model is genuinely uncertain\n",
    "- A/B testing: quantify uncertainty in conversion rate predictions\n",
    "- Any binary outcome where knowing *how confident the model is* matters\n",
    "\n",
    "**How it works**: The predictive probability for class 1 is `alpha / (alpha + beta)`.\n",
    "When alpha and beta are both large, the model is confident. When they're small,\n",
    "there's high uncertainty.\n",
    "\n",
    "This notebook shows the **classification** pattern using `make_sympy_log_score`\n",
    "(the lower-level factory) with a thin `ClassificationDistn` wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')  # add project root so 'import ngboost' finds the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "import sympy.stats as symstats\n",
    "from scipy.special import digamma\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ngboost import NGBClassifier\n",
    "from ngboost.distns.distn import ClassificationDistn\n",
    "from ngboost.distns.sympy_utils import make_sympy_log_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the score class\n",
    "\n",
    "The BetaBernoulli uses a Beta(alpha, beta) prior, so the predictive probability\n",
    "is `p = alpha / (alpha + beta)`. The likelihood is Bernoulli(p), and the factory\n",
    "auto-derives score, gradient, and Fisher Information from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score class: <class 'ngboost.distns.sympy_utils.BetaBernoulliLogScore'>\n",
      "  Has analytical metric: True\n"
     ]
    }
   ],
   "source": [
    "alpha, beta, y = sp.symbols(\"alpha beta y\")\n",
    "p = alpha / (alpha + beta)\n",
    "\n",
    "BetaBernoulliLogScore = make_sympy_log_score(\n",
    "    params=[(alpha, True), (beta, True)],\n",
    "    y=y,\n",
    "    sympy_dist=symstats.Bernoulli(\"Y\", p),\n",
    "    name=\"BetaBernoulliLogScore\",\n",
    ")\n",
    "\n",
    "print(f\"Score class: {BetaBernoulliLogScore}\")\n",
    "print(f\"  Has analytical metric: {'metric' in BetaBernoulliLogScore.__dict__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the classification distribution\n",
    "\n",
    "For classification, NGBoost requires a `ClassificationDistn` subclass with a\n",
    "`class_probs()` method. This is a thin wrapper — the heavy lifting (score,\n",
    "gradient, FI) is handled by the factory-generated score class above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution: <class '__main__.BetaBernoulli'>\n",
      "  Base: (<class 'ngboost.distns.distn.ClassificationDistn'>,)\n"
     ]
    }
   ],
   "source": [
    "class BetaBernoulli(ClassificationDistn):\n",
    "\n",
    "    n_params = 2\n",
    "    scores = [BetaBernoulliLogScore]\n",
    "\n",
    "    def __init__(self, params):\n",
    "        super().__init__(params)\n",
    "        self.alpha = np.exp(params[0])\n",
    "        self.beta = np.exp(params[1])\n",
    "\n",
    "    def class_probs(self):\n",
    "        p = self.alpha / (self.alpha + self.beta)\n",
    "        return np.column_stack([1 - p, p])\n",
    "\n",
    "    def fit(Y):\n",
    "        p = np.clip(np.mean(Y), 0.01, 0.99)\n",
    "        a, b = p * 2, (1 - p) * 2\n",
    "        for _ in range(100):\n",
    "            ab = a + b\n",
    "            psi_ab = digamma(ab)\n",
    "            a = np.clip(a * (np.mean(digamma(Y + a)) - psi_ab) / (digamma(a) - psi_ab + 1e-10), 1e-4, 1e4)\n",
    "            b = np.clip(b * (np.mean(digamma(1 - Y + b)) - psi_ab) / (digamma(b) - psi_ab + 1e-10), 1e-4, 1e4)\n",
    "        return np.array([np.log(a), np.log(b)])\n",
    "\n",
    "    def sample(self, m):\n",
    "        p = np.squeeze(self.alpha / (self.alpha + self.beta))\n",
    "        return np.random.binomial(1, p, size=m).astype(float)\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return {\"alpha\": self.alpha, \"beta\": self.beta}\n",
    "\n",
    "\n",
    "print(f\"Distribution: {BetaBernoulli}\")\n",
    "print(f\"  Base: {BetaBernoulli.__bases__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load data\n",
    "\n",
    "We use the breast cancer dataset — a standard binary classification benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 455 (62.9% positive)\n",
      "Test:  114 (62.3% positive)\n"
     ]
    }
   ],
   "source": [
    "X, Y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42\n",
    ")\n",
    "Y_train = Y_train.astype(float)\n",
    "Y_test = Y_test.astype(float)\n",
    "\n",
    "print(f\"Train: {len(Y_train)} ({Y_train.mean():.1%} positive)\")\n",
    "print(f\"Test:  {len(Y_test)} ({Y_test.mean():.1%} positive)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train NGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guyko\\AppData\\Local\\Temp\\ipykernel_15172\\1805550954.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  self.alpha = np.exp(params[0])\n",
      "C:\\Users\\guyko\\AppData\\Local\\Temp\\ipykernel_15172\\1805550954.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  self.beta = np.exp(params[1])\n",
      "C:\\Users\\guyko\\AppData\\Local\\Temp\\ipykernel_15172\\1805550954.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  self.alpha = np.exp(params[0])\n",
      "C:\\Users\\guyko\\AppData\\Local\\Temp\\ipykernel_15172\\1805550954.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  self.beta = np.exp(params[1])\n",
      "C:\\Users\\guyko\\AppData\\Local\\Temp\\ipykernel_15172\\1805550954.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  self.alpha = np.exp(params[0])\n",
      "C:\\Users\\guyko\\AppData\\Local\\Temp\\ipykernel_15172\\1805550954.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  self.beta = np.exp(params[1])\n",
      "C:\\Users\\guyko\\AppData\\Local\\Temp\\ipykernel_15172\\1805550954.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  self.alpha = np.exp(params[0])\n",
      "C:\\Users\\guyko\\AppData\\Local\\Temp\\ipykernel_15172\\1805550954.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  self.beta = np.exp(params[1])\n",
      "C:\\Users\\guyko\\AppData\\Local\\Temp\\ipykernel_15172\\1805550954.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  self.alpha = np.exp(params[0])\n",
      "C:\\Users\\guyko\\AppData\\Local\\Temp\\ipykernel_15172\\1805550954.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  self.beta = np.exp(params[1])\n",
      "<lambdifygenerated-14>:2: RuntimeWarning: overflow encountered in multiply\n",
      "  return Dummy_106*Dummy_107/(Dummy_106 + Dummy_107)**2\n",
      "<lambdifygenerated-14>:2: RuntimeWarning: overflow encountered in square\n",
      "  return Dummy_106*Dummy_107/(Dummy_106 + Dummy_107)**2\n",
      "<lambdifygenerated-14>:2: RuntimeWarning: invalid value encountered in divide\n",
      "  return Dummy_106*Dummy_107/(Dummy_106 + Dummy_107)**2\n",
      "<lambdifygenerated-15>:2: RuntimeWarning: overflow encountered in multiply\n",
      "  return -Dummy_110*Dummy_111/(Dummy_110 + Dummy_111)**2\n",
      "<lambdifygenerated-15>:2: RuntimeWarning: overflow encountered in square\n",
      "  return -Dummy_110*Dummy_111/(Dummy_110 + Dummy_111)**2\n",
      "<lambdifygenerated-15>:2: RuntimeWarning: invalid value encountered in divide\n",
      "  return -Dummy_110*Dummy_111/(Dummy_110 + Dummy_111)**2\n",
      "<lambdifygenerated-16>:2: RuntimeWarning: overflow encountered in multiply\n",
      "  return -Dummy_114*Dummy_115/(Dummy_114 + Dummy_115)**2\n",
      "<lambdifygenerated-16>:2: RuntimeWarning: overflow encountered in square\n",
      "  return -Dummy_114*Dummy_115/(Dummy_114 + Dummy_115)**2\n",
      "<lambdifygenerated-16>:2: RuntimeWarning: invalid value encountered in divide\n",
      "  return -Dummy_114*Dummy_115/(Dummy_114 + Dummy_115)**2\n",
      "<lambdifygenerated-17>:2: RuntimeWarning: overflow encountered in multiply\n",
      "  return Dummy_118*Dummy_119/(Dummy_118**2 + 2*Dummy_118*Dummy_119 + Dummy_119**2)\n",
      "<lambdifygenerated-17>:2: RuntimeWarning: overflow encountered in square\n",
      "  return Dummy_118*Dummy_119/(Dummy_118**2 + 2*Dummy_118*Dummy_119 + Dummy_119**2)\n",
      "<lambdifygenerated-17>:2: RuntimeWarning: invalid value encountered in divide\n",
      "  return Dummy_118*Dummy_119/(Dummy_118**2 + 2*Dummy_118*Dummy_119 + Dummy_119**2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m tree_learner \u001b[38;5;241m=\u001b[39m DecisionTreeRegressor(\n\u001b[0;32m      3\u001b[0m     criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfriedman_mse\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     min_samples_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m ngb \u001b[38;5;241m=\u001b[39m NGBClassifier(\n\u001b[0;32m     12\u001b[0m     Dist\u001b[38;5;241m=\u001b[39mBetaBernoulli,\n\u001b[0;32m     13\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     Base\u001b[38;5;241m=\u001b[39mtree_learner\n\u001b[0;32m     18\u001b[0m )\n\u001b[1;32m---> 19\u001b[0m \u001b[43mngb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\work\\ngboost\\notebooks\\..\\ngboost\\ngboost.py:258\u001b[0m, in \u001b[0;36mNGBoost.fit\u001b[1;34m(self, X, Y, X_val, Y_val, sample_weight, val_sample_weight, train_loss_monitor, val_loss_monitor, early_stopping_rounds)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscalings \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol_idxs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mY_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loss_monitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loss_monitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loss_monitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loss_monitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\work\\ngboost\\notebooks\\..\\ngboost\\ngboost.py:435\u001b[0m, in \u001b[0;36mNGBoost.partial_fit\u001b[1;34m(self, X, Y, X_val, Y_val, sample_weight, val_sample_weight, train_loss_monitor, val_loss_monitor, early_stopping_rounds)\u001b[0m\n\u001b[0;32m    432\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_list[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    433\u001b[0m grads \u001b[38;5;241m=\u001b[39m D\u001b[38;5;241m.\u001b[39mgrad(Y_batch, natural\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnatural_gradient)\n\u001b[1;32m--> 435\u001b[0m proj_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_base\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_search(proj_grad, P_batch, Y_batch, weight_batch)\n\u001b[0;32m    438\u001b[0m params \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;241m*\u001b[39m scale\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39marray([m\u001b[38;5;241m.\u001b[39mpredict(X[:, col_idx]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_models[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    442\u001b[0m )\n",
      "File \u001b[1;32md:\\work\\ngboost\\notebooks\\..\\ngboost\\ngboost.py:172\u001b[0m, in \u001b[0;36mNGBoost.fit_base\u001b[1;34m(self, X, grads, sample_weight)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_base\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, grads, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m         models \u001b[38;5;241m=\u001b[39m [clone(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBase)\u001b[38;5;241m.\u001b[39mfit(X, g) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads\u001b[38;5;241m.\u001b[39mT]\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    174\u001b[0m         models \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    175\u001b[0m             clone(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBase)\u001b[38;5;241m.\u001b[39mfit(X, g, sample_weight\u001b[38;5;241m=\u001b[39msample_weight) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    176\u001b[0m         ]\n",
      "File \u001b[1;32md:\\work\\ngboost\\notebooks\\..\\ngboost\\ngboost.py:172\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_base\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, grads, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m         models \u001b[38;5;241m=\u001b[39m [\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBase\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads\u001b[38;5;241m.\u001b[39mT]\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    174\u001b[0m         models \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    175\u001b[0m             clone(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBase)\u001b[38;5;241m.\u001b[39mfit(X, g, sample_weight\u001b[38;5;241m=\u001b[39msample_weight) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    176\u001b[0m         ]\n",
      "File \u001b[1;32md:\\Users\\guyko\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Users\\guyko\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:1404\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m \n\u001b[0;32m   1378\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1404\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1406\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32md:\\Users\\guyko\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:252\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    248\u001b[0m check_X_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    249\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mDTYPE, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, ensure_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    250\u001b[0m )\n\u001b[0;32m    251\u001b[0m check_y_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 252\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_separately\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_y_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m missing_values_in_feature_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_missing_values_in_feature_mask(X)\n\u001b[0;32m    258\u001b[0m )\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n",
      "File \u001b[1;32md:\\Users\\guyko\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:2959\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2957\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_y_params:\n\u001b[0;32m   2958\u001b[0m         check_y_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_check_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params}\n\u001b[1;32m-> 2959\u001b[0m     y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m   2960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2961\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32md:\\Users\\guyko\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1107\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[1;32m-> 1107\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1116\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\guyko\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\guyko\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_learner = DecisionTreeRegressor(\n",
    "    criterion=\"friedman_mse\",\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=3,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_depth=6,\n",
    "    splitter=\"best\",\n",
    "    random_state=None,\n",
    ")\n",
    "ngb = NGBClassifier(\n",
    "    Dist=BetaBernoulli,\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    verbose=False,\n",
    "    random_state=42,\n",
    "    Base=tree_learner\n",
    ")\n",
    "ngb.fit(X_train, Y_train)\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predictions with uncertainty\n",
    "\n",
    "The key advantage: not just a probability, but a *distribution* over probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class predictions\n",
    "Y_pred = ngb.predict(X_test)\n",
    "accuracy = np.mean(Y_pred == Y_test)\n",
    "print(f\"Accuracy: {accuracy:.1%}\")\n",
    "\n",
    "# Predicted probability distributions\n",
    "dists = ngb.pred_dist(X_test)\n",
    "probs = dists.class_probs()  # (n_samples, 2) — [P(0), P(1)]\n",
    "\n",
    "print(f\"\\nFirst 5 test samples:\")\n",
    "print(f\"  alpha:  {dists.alpha[:5].round(3)}\")\n",
    "print(f\"  beta:   {dists.beta[:5].round(3)}\")\n",
    "print(f\"  P(y=1): {probs[:5, 1].round(4)}\")\n",
    "print(f\"  Actual: {Y_test[:5].astype(int)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Quantify prediction uncertainty\n",
    "\n",
    "The Beta distribution over `p` lets us compute confidence intervals.\n",
    "When alpha and beta are large, the model is confident. When they're\n",
    "small, there's high uncertainty about the true probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "# Compute 90% credible intervals for P(y=1)\n",
    "p_lower = scipy.stats.beta.ppf(0.05, dists.alpha, dists.beta)\n",
    "p_upper = scipy.stats.beta.ppf(0.95, dists.alpha, dists.beta)\n",
    "p_mean = dists.alpha / (dists.alpha + dists.beta)\n",
    "\n",
    "# Width of credible interval = uncertainty\n",
    "uncertainty = p_upper - p_lower\n",
    "\n",
    "print(\"90% credible intervals for P(y=1) — first 10 test samples:\")\n",
    "print(f\"{'idx':>4s}  {'P(y=1)':>8s}  {'90% CI':>18s}  {'width':>8s}  {'true':>5s}\")\n",
    "for i in range(10):\n",
    "    print(f\"{i:4d}  {p_mean[i]:8.4f}  [{p_lower[i]:.4f}, {p_upper[i]:.4f}]  {uncertainty[i]:8.4f}  {int(Y_test[i]):5d}\")\n",
    "\n",
    "print(f\"\\nMean uncertainty (CI width): {uncertainty.mean():.4f}\")\n",
    "print(f\"Most uncertain sample:  idx={np.argmax(uncertainty)}, width={uncertainty.max():.4f}\")\n",
    "print(f\"Most confident sample:  idx={np.argmin(uncertainty)}, width={uncertainty.min():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: predicted probabilities with credible intervals, sorted by predicted P(y=1)\n",
    "ax = axes[0]\n",
    "order = np.argsort(p_mean)\n",
    "x = np.arange(len(order))\n",
    "ax.fill_between(x, p_lower[order], p_upper[order], alpha=0.3, label=\"90% CI\")\n",
    "ax.plot(x, p_mean[order], \"b-\", linewidth=0.8, label=\"P(y=1)\")\n",
    "ax.scatter(x, Y_test[order], c=\"red\", s=8, alpha=0.6, zorder=5, label=\"true label\")\n",
    "ax.set_xlabel(\"Test samples (sorted by P(y=1))\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "ax.set_title(\"Predictions with 90% credible intervals\")\n",
    "ax.legend()\n",
    "\n",
    "# Right: Beta distributions for a confident vs uncertain prediction\n",
    "ax = axes[1]\n",
    "p_grid = np.linspace(0.001, 0.999, 300)\n",
    "\n",
    "# Most confident\n",
    "i_conf = np.argmin(uncertainty)\n",
    "pdf_conf = scipy.stats.beta.pdf(p_grid, dists.alpha[i_conf], dists.beta[i_conf])\n",
    "ax.plot(p_grid, pdf_conf,\n",
    "        label=f\"confident (idx={i_conf}, true={int(Y_test[i_conf])})\")\n",
    "\n",
    "# Most uncertain\n",
    "i_unc = np.argmax(uncertainty)\n",
    "pdf_unc = scipy.stats.beta.pdf(p_grid, dists.alpha[i_unc], dists.beta[i_unc])\n",
    "ax.plot(p_grid, pdf_unc,\n",
    "        label=f\"uncertain (idx={i_unc}, true={int(Y_test[i_unc])})\")\n",
    "\n",
    "ax.set_xlabel(\"P(y=1)\")\n",
    "ax.set_ylabel(\"density\")\n",
    "ax.set_title(\"Beta prior over probability\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: classification vs regression\n",
    "\n",
    "For **classification**, `make_distribution` doesn't apply directly (it creates\n",
    "`RegressionDistn`). Instead, use the lower-level `make_sympy_log_score` to generate\n",
    "the score class, then write a thin `ClassificationDistn` wrapper with `class_probs()`.\n",
    "\n",
    "The score class still handles all the calculus automatically — you only need to\n",
    "write `__init__`, `class_probs`, `fit`, and `sample` (all straightforward)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
