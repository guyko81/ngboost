{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Normal Distribution with NGBoost (SymPy Factory)\n\n**Problem**: Standard regression with uncertainty â€” predict a continuous outcome\nand get calibrated prediction intervals, not just a point estimate.\n\n**When to use**:\n- House price prediction: \"this house is worth $350k, with 90% chance between $300-400k\"\n- Temperature forecasting: \"tomorrow's high will be 25C +/- 3C\"\n- Any continuous outcome that is roughly symmetric and unbounded\n\n**What this notebook shows**: How to create a Normal distribution from scratch\nusing `make_distribution`, train NGBoost with it, and compare against NGBoost's\nbuilt-in `Normal` to verify the results are equivalent."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "import sympy.stats as symstats\n",
    "import scipy.stats\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ngboost import NGBRegressor\n",
    "from ngboost.distns import Normal  # built-in\n",
    "from ngboost.distns.sympy_utils import make_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the Normal distribution\n",
    "\n",
    "Three ingredients:\n",
    "1. **SymPy symbols** for the parameters and observed variable\n",
    "2. **`sympy.stats` distribution** for auto-deriving score, gradient, and Fisher Information\n",
    "3. **`scipy.stats` distribution** for fitting, sampling, quantiles, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: define symbolic parameters\n",
    "loc, scale, y = sp.symbols(\"loc scale y\", positive=True)\n",
    "\n",
    "# Step 2: one call creates the full distribution\n",
    "SympyNormal = make_distribution(\n",
    "    params=[(loc, False), (scale, True)],   # loc: identity link, scale: log link\n",
    "    y=y,\n",
    "    sympy_dist=symstats.Normal(\"Y\", loc, scale),\n",
    "    scipy_dist_cls=scipy.stats.norm,\n",
    "    scipy_kwarg_map={\"loc\": loc, \"scale\": scale},\n",
    "    name=\"SympyNormal\",\n",
    ")\n",
    "\n",
    "print(f\"Created: {SympyNormal}\")\n",
    "print(f\"  n_params: {SympyNormal.n_params}\")\n",
    "print(f\"  scores:   {SympyNormal.scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "print(f\"Train: {X_train.shape[0]} samples, Test: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train NGBoost\n",
    "\n",
    "Train two models: one with the SymPy-generated Normal, one with the built-in Normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngb_sympy = NGBRegressor(Dist=SympyNormal, n_estimators=200, verbose=False, random_state=42)\n",
    "ngb_sympy.fit(X_train, Y_train)\n",
    "\n",
    "ngb_builtin = NGBRegressor(Dist=Normal, n_estimators=200, verbose=False, random_state=42)\n",
    "ngb_builtin.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Both models trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sympy = ngb_sympy.predict(X_test)\n",
    "pred_builtin = ngb_builtin.predict(X_test)\n",
    "\n",
    "print(\"Point predictions (first 5):\")\n",
    "print(f\"  SymPy:    {pred_sympy[:5].round(2)}\")\n",
    "print(f\"  Built-in: {pred_builtin[:5].round(2)}\")\n",
    "print(f\"  Actual:   {Y_test[:5].round(2)}\")\n",
    "print(f\"\\nMax difference: {np.max(np.abs(pred_sympy - pred_builtin)):.6f}\")\n",
    "print(f\"All close: {np.allclose(pred_sympy, pred_builtin, atol=1e-2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predicted distributions\n",
    "\n",
    "NGBoost returns full distribution objects. Access parameters, quantiles, CDF, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = ngb_sympy.pred_dist(X_test)\n",
    "\n",
    "# Access estimated parameters\n",
    "print(\"Estimated parameters (first 5 test samples):\")\n",
    "print(f\"  loc:   {dists.loc[:5].round(2)}\")\n",
    "print(f\"  scale: {dists.scale[:5].round(2)}\")\n",
    "\n",
    "# Quantiles\n",
    "q10 = dists.ppf(0.1)\n",
    "q50 = dists.ppf(0.5)\n",
    "q90 = dists.ppf(0.9)\n",
    "print(f\"\\nQuantiles (first 5):\")\n",
    "print(f\"  10th: {q10[:5].round(2)}\")\n",
    "print(f\"  50th: {q50[:5].round(2)}\")\n",
    "print(f\"  90th: {q90[:5].round(2)}\")\n",
    "\n",
    "# CDF: P(Y <= actual)\n",
    "cdf_vals = dists.cdf(Y_test)\n",
    "print(f\"\\nCDF at true values (first 5): {cdf_vals[:5].round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# Left: predictions vs actuals\n",
    "ax = axes[0]\n",
    "ax.scatter(Y_test, pred_sympy, alpha=0.6, s=30)\n",
    "ax.plot([Y_test.min(), Y_test.max()], [Y_test.min(), Y_test.max()], \"r--\")\n",
    "ax.set_xlabel(\"Actual\")\n",
    "ax.set_ylabel(\"Predicted (mean)\")\n",
    "ax.set_title(\"Predicted vs Actual\")\n",
    "\n",
    "# Right: predicted distributions for 3 samples\n",
    "ax = axes[1]\n",
    "for i in [0, 20, 50]:\n",
    "    mu_i, sigma_i = dists.loc[i], dists.scale[i]\n",
    "    x = np.linspace(mu_i - 3 * sigma_i, mu_i + 3 * sigma_i, 200)\n",
    "    ax.plot(x, scipy.stats.norm.pdf(x, mu_i, sigma_i), label=f\"sample {i}\")\n",
    "    ax.axvline(Y_test[i], color=ax.lines[-1].get_color(), linestyle=\":\", alpha=0.5)\n",
    "ax.set_xlabel(\"y\")\n",
    "ax.set_ylabel(\"density\")\n",
    "ax.set_title(\"Predicted distributions (3 test samples)\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The SymPy-generated Normal is fully equivalent to the built-in one.\n",
    "The recipe is always the same:\n",
    "\n",
    "```python\n",
    "MyDist = make_distribution(\n",
    "    params=[...],          # (symbol, log_transformed) pairs\n",
    "    y=y_symbol,\n",
    "    sympy_dist=...,        # sympy.stats distribution\n",
    "    scipy_dist_cls=...,    # scipy.stats class\n",
    "    scipy_kwarg_map=...,   # {scipy_name: sympy_symbol}\n",
    "    name=\"MyDist\",\n",
    ")\n",
    "\n",
    "ngb = NGBRegressor(Dist=MyDist)\n",
    "ngb.fit(X, Y)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}