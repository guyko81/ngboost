{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SymPy Factory Demo: Normal Distribution\n",
    "\n",
    "This notebook demonstrates `make_sympy_log_score` for the **Normal** distribution.\n",
    "\n",
    "**The key idea**: you just define a `sympy.stats` distribution, and the factory\n",
    "auto-derives `score()`, `d_score()`, and `metric()` — no manual calculus needed.\n",
    "\n",
    "We then peek under the hood at the symbolic expressions, and verify everything\n",
    "matches the hand-written `NormalLogScore` on real samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, '..')  # add project root so 'import ngboost' finds the package\n\nimport numpy as np\nimport sympy as sp\nimport sympy.stats as symstats\nfrom IPython.display import Math, display\n\nsp.init_printing(use_latex='mathjax')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the distribution and call the factory\n",
    "\n",
    "This is **all you need**. Define a `sympy.stats` distribution, specify which\n",
    "parameters are log-transformed, and the factory does the rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add ngboost path to sys.path\n",
    "import sys\n",
    "sys.path.append('../ngboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ngboost.distns'; 'ngboost' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mngboost\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistns\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msympy_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_sympy_log_score\n\u001b[0;32m      3\u001b[0m loc, scale, y \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39msymbols(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloc scale y\u001b[39m\u001b[38;5;124m'\u001b[39m, positive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Just define the distribution — no manual log-likelihood needed!\u001b[39;00m\n",
      "File \u001b[1;32md:\\work\\ngboost\\notebooks\\../ngboost\\ngboost.py:13\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeRegressor\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_array, check_random_state, check_X_y\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mngboost\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistns\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultivariateNormal, Normal, k_categorical\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mngboost\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlearners\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m default_tree_learner\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mngboost\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanifold\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m manifold\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ngboost.distns'; 'ngboost' is not a package"
     ]
    }
   ],
   "source": [
    "from ngboost.distns.sympy_utils import make_sympy_log_score\n",
    "\n",
    "loc, scale, y = sp.symbols('loc scale y', positive=True)\n",
    "\n",
    "# Just define the distribution — no manual log-likelihood needed!\n",
    "SympyNormalLogScore = make_sympy_log_score(\n",
    "    params=[(loc, False), (scale, True)],  # loc: identity, scale: log-link\n",
    "    y=y,\n",
    "    sympy_dist=symstats.Normal('Y', loc, scale),\n",
    "    name='SympyNormalLogScore',\n",
    ")\n",
    "\n",
    "print(f'Generated class:  {SympyNormalLogScore}')\n",
    "print(f'Base classes:     {[c.__name__ for c in SympyNormalLogScore.__mro__]}')\n",
    "print(f'Has score():      {\"score\" in SympyNormalLogScore.__dict__}')\n",
    "print(f'Has d_score():    {\"d_score\" in SympyNormalLogScore.__dict__}')\n",
    "print(f'Has metric():     {\"metric\" in SympyNormalLogScore.__dict__}  (analytical FI)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Under the hood: what the factory derives\n",
    "\n",
    "Let's inspect the symbolic expressions the factory computes internally.\n",
    "\n",
    "### 2a. Score expression (auto-derived from distribution)\n",
    "\n",
    "The factory calls `sympy.stats.density(dist)(y)` to get the PDF, then takes $-\\log$:\n",
    "\n",
    "$$\\text{score}(y) = -\\log p(y \\mid \\mu, \\sigma) = \\tfrac{1}{2}\\log(2\\pi) + \\log\\sigma + \\frac{(y - \\mu)^2}{2\\sigma^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what the factory computes internally:\n",
    "dist = symstats.Normal('Y', loc, scale)\n",
    "pdf = symstats.density(dist)(y)\n",
    "score_expr = -sp.log(pdf)\n",
    "\n",
    "display(Math(r'\\text{PDF} = ' + sp.latex(pdf)))\n",
    "display(Math(r'\\text{score}(y) = -\\log(\\text{PDF}) = ' + sp.latex(sp.simplify(score_expr))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Gradients (auto-derived via `sym.diff` + chain rule)\n",
    "\n",
    "NGBoost parameterises `loc` with an **identity** link and `scale` with a **log** link.\n",
    "\n",
    "| parameter | internal param | chain rule |\n",
    "|-----------|---------------|------------|\n",
    "| `loc`     | $\\mu$         | $\\frac{\\partial}{\\partial \\mu}$ (identity) |\n",
    "| `scale`   | $\\log\\sigma$  | $\\sigma \\cdot \\frac{\\partial}{\\partial \\sigma}$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw derivative w.r.t. loc (identity link)\n",
    "grad_loc = sp.simplify(sp.diff(score_expr, loc))\n",
    "\n",
    "# Raw derivative w.r.t. scale, then chain rule for log link\n",
    "grad_logscale = sp.simplify(scale * sp.diff(score_expr, scale))\n",
    "\n",
    "display(Math(r'\\frac{\\partial\\, \\text{score}}{\\partial\\, \\mu} = ' + sp.latex(grad_loc)))\n",
    "display(Math(r'\\frac{\\partial\\, \\text{score}}{\\partial\\, \\log\\sigma} = \\sigma \\cdot \\frac{\\partial\\, \\text{score}}{\\partial\\, \\sigma} = ' + sp.latex(grad_logscale)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Fisher Information (auto-derived via Hessian + expectation)\n",
    "\n",
    "The factory computes the raw-parameter Hessian $H_{ij} = \\frac{\\partial^2 \\text{score}}{\\partial\\theta_i\\,\\partial\\theta_j}$, then checks whether each element depends on $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = sp.Matrix([\n",
    "    [sp.diff(score_expr, loc, loc),   sp.diff(score_expr, loc, scale)],\n",
    "    [sp.diff(score_expr, scale, loc), sp.diff(score_expr, scale, scale)],\n",
    "])\n",
    "H_simplified = sp.simplify(H)\n",
    "\n",
    "print('Raw-parameter Hessian:')\n",
    "display(H_simplified)\n",
    "\n",
    "print()\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        has_y = y in H_simplified[i, j].free_symbols\n",
    "        print(f'  H[{i},{j}] depends on y: {has_y}')\n",
    "\n",
    "print()\n",
    "print('H[0,0] is y-free, but H[0,1] and H[1,1] depend on y.')\n",
    "print('The factory falls through to tier 2: substitute y -> Y ~ N(mu,sigma) and take E[H].')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y_rv = symstats.Normal('Y', loc, scale)\n",
    "\n",
    "E_H = sp.zeros(2, 2)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        hij = H_simplified[i, j]\n",
    "        if y not in hij.free_symbols:\n",
    "            E_H[i, j] = hij\n",
    "        else:\n",
    "            E_H[i, j] = sp.simplify(symstats.E(hij.subs(y, Y_rv)))\n",
    "\n",
    "print('E[Hessian] in raw-parameter space:')\n",
    "display(E_H)\n",
    "\n",
    "# Transform to internal-parameter space via Jacobian J = diag(1, sigma)\n",
    "J = [1, scale]\n",
    "FI = sp.zeros(2, 2)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        FI[i, j] = sp.simplify(J[i] * J[j] * E_H[i, j])\n",
    "\n",
    "print('\\nFisher Information in internal-parameter space (loc, log scale):')\n",
    "display(FI)\n",
    "print()\n",
    "print('This matches the hand-written NormalLogScore.metric():')\n",
    "print('  FI[0,0] = 1/sigma^2,  FI[1,1] = 2,  FI[0,1] = FI[1,0] = 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare against hand-written implementation\n",
    "\n",
    "Create a Normal distribution, draw samples, and evaluate score / d_score / metric\n",
    "using **both** the SymPy-generated and hand-written score classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngboost.distns import Normal\n",
    "from ngboost.distns.normal import NormalLogScore as HandWrittenNormalLogScore\n",
    "from ngboost.manifold import manifold\n",
    "from ngboost.scores import LogScore\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters: loc=2.0, scale=exp(log(1.5))=1.5\n",
    "params = np.array([[2.0], [np.log(1.5)]])\n",
    "\n",
    "# Hand-written manifold\n",
    "M = manifold(LogScore, Normal)\n",
    "hw = M(params)\n",
    "\n",
    "# SymPy-generated score object (shares the same distribution state)\n",
    "sy = SympyNormalLogScore.__new__(SympyNormalLogScore)\n",
    "sy.__dict__.update(hw.__dict__)\n",
    "\n",
    "print(f'Distribution parameters:')\n",
    "print(f'  loc   = {hw.loc}')\n",
    "print(f'  scale = {hw.scale}')\n",
    "print(f'  var   = {hw.var}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw samples\n",
    "samples = hw.sample(8)\n",
    "print('Samples from Normal(loc=2.0, scale=1.5):')\n",
    "print(samples)\n",
    "print(f'  shape: {samples.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Score (negative log-likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_hw = hw.score(samples)\n",
    "score_sy = sy.score(samples)\n",
    "\n",
    "print('score(y) — negative log-likelihood per sample:')\n",
    "print(f'  {\"Sample\":>10s}  {\"Hand-written\":>14s}  {\"SymPy\":>14s}  {\"Match\":>6s}')\n",
    "print(f'  {\"-\"*10}  {\"-\"*14}  {\"-\"*14}  {\"-\"*6}')\n",
    "for i in range(len(samples)):\n",
    "    match = '  yes' if np.isclose(score_hw[i], score_sy[i]) else '  NO'\n",
    "    print(f'  {samples[i]:10.4f}  {score_hw[i]:14.6f}  {score_sy[i]:14.6f}  {match}')\n",
    "\n",
    "print(f'\\nAll close: {np.allclose(score_hw, score_sy)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Gradient (d_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dscore_hw = hw.d_score(samples)\n",
    "dscore_sy = sy.d_score(samples)\n",
    "\n",
    "print('d_score(y) — gradient of score w.r.t. internal parameters:')\n",
    "print(f'  {\"Sample\":>10s}  {\"d/d(loc) HW\":>14s}  {\"d/d(loc) Sy\":>14s}  {\"d/d(logσ) HW\":>14s}  {\"d/d(logσ) Sy\":>14s}')\n",
    "print(f'  {\"-\"*10}  {\"-\"*14}  {\"-\"*14}  {\"-\"*14}  {\"-\"*14}')\n",
    "for i in range(len(samples)):\n",
    "    print(f'  {samples[i]:10.4f}  {dscore_hw[i,0]:14.6f}  {dscore_sy[i,0]:14.6f}  {dscore_hw[i,1]:14.6f}  {dscore_sy[i,1]:14.6f}')\n",
    "\n",
    "print(f'\\nAll close: {np.allclose(dscore_hw, dscore_sy)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Metric (Fisher Information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_hw = hw.metric()\n",
    "metric_sy = sy.metric()\n",
    "\n",
    "print('metric() — Fisher Information matrix (analytical):')\n",
    "print()\n",
    "print('Hand-written:')\n",
    "print(metric_hw[0])\n",
    "print()\n",
    "print('SymPy-generated:')\n",
    "print(metric_sy[0])\n",
    "print()\n",
    "print(f'Expected: [[1/var, 0], [0, 2]] = [[{1/hw.var[0]:.4f}, 0], [0, 2]]')\n",
    "print(f'All close: {np.allclose(metric_hw, metric_sy)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d. Verify gradient with finite differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import approx_fprime\n",
    "\n",
    "# Pick one sample\n",
    "y_test = samples[:1]\n",
    "\n",
    "# Finite-difference gradient\n",
    "def score_fn(p):\n",
    "    return M(p.reshape(-1, 1)).score(y_test)\n",
    "\n",
    "grad_fd = approx_fprime(params.flatten(), score_fn, 1e-6)\n",
    "grad_analytical = sy.d_score(y_test).flatten()\n",
    "\n",
    "print(f'Sample y = {y_test[0]:.4f}')\n",
    "print(f'  Finite-diff gradient:  {grad_fd}')\n",
    "print(f'  Analytical gradient:   {grad_analytical}')\n",
    "print(f'  Abs error:             {np.abs(grad_fd - grad_analytical)}')\n",
    "print(f'  Match (tol=1e-4):      {np.allclose(grad_fd, grad_analytical, atol=1e-4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3e. Verify metric with Monte Carlo estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo estimate of Fisher Information\n",
    "n_mc = 50000\n",
    "mc_samples = hw.sample(n_mc)\n",
    "grads = np.stack([sy.d_score(np.array([s])) for s in mc_samples])\n",
    "metric_mc = np.mean(np.einsum('sik,sij->sijk', grads, grads), axis=0)\n",
    "\n",
    "print('Metric comparison (analytical vs Monte Carlo):')\n",
    "print()\n",
    "print('Analytical:')\n",
    "print(metric_sy[0])\n",
    "print()\n",
    "print(f'Monte Carlo ({n_mc:,} samples):')\n",
    "print(metric_mc[0])\n",
    "print()\n",
    "rel_err = np.linalg.norm(metric_mc - metric_sy) / np.linalg.norm(metric_sy)\n",
    "print(f'Relative error: {rel_err:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualise score and gradients across $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_range = np.linspace(-2, 6, 200)\n",
    "scores_over_y = sy.score(y_range)\n",
    "dscores_over_y = sy.d_score(y_range)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Score\n",
    "axes[0].plot(y_range, scores_over_y, 'b-', linewidth=2)\n",
    "axes[0].scatter(samples, sy.score(samples), c='red', s=60, zorder=5, label='samples')\n",
    "axes[0].set_xlabel('y')\n",
    "axes[0].set_ylabel('score(y)')\n",
    "axes[0].set_title('Negative log-likelihood')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# d_score / d(loc)\n",
    "axes[1].plot(y_range, dscores_over_y[:, 0], 'g-', linewidth=2)\n",
    "axes[1].scatter(samples, sy.d_score(samples)[:, 0], c='red', s=60, zorder=5, label='samples')\n",
    "axes[1].axhline(0, color='k', linewidth=0.5, linestyle='--')\n",
    "axes[1].set_xlabel('y')\n",
    "axes[1].set_ylabel(r'd score / d $\\mu$')\n",
    "axes[1].set_title(r'Gradient w.r.t. $\\mu$ (identity link)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# d_score / d(log scale)\n",
    "axes[2].plot(y_range, dscores_over_y[:, 1], 'm-', linewidth=2)\n",
    "axes[2].scatter(samples, sy.d_score(samples)[:, 1], c='red', s=60, zorder=5, label='samples')\n",
    "axes[2].axhline(0, color='k', linewidth=0.5, linestyle='--')\n",
    "axes[2].set_xlabel('y')\n",
    "axes[2].set_ylabel(r'd score / d $\\log\\sigma$')\n",
    "axes[2].set_title(r'Gradient w.r.t. $\\log\\sigma$ (log link)')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle(r'Normal($\\mu$=2.0, $\\sigma$=1.5) — SymPy-generated score functions', fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "You just define a `sympy.stats` distribution and the factory does everything:\n",
    "\n",
    "```python\n",
    "SympyNormalLogScore = make_sympy_log_score(\n",
    "    params=[(loc, False), (scale, True)],\n",
    "    y=y,\n",
    "    sympy_dist=symstats.Normal('Y', loc, scale),\n",
    ")\n",
    "```\n",
    "\n",
    "The factory automatically:\n",
    "\n",
    "| Method | What it computes | How |\n",
    "|--------|-----------------|-----|\n",
    "| `score(Y)` | $-\\log p(y \\mid \\theta)$ | Auto-derives from `sympy.stats.density`, lambdifies |\n",
    "| `d_score(Y)` | $\\nabla_{\\eta}\\, \\text{score}(y)$ | `sym.diff` + chain rule for log-params, lambdified |\n",
    "| `metric()` | Fisher Information $I(\\theta)$ | y-free Hessian or $\\mathbb{E}[\\text{Hessian}]$ via `sympy.stats`, lambdified |\n",
    "\n",
    "All three match the hand-written `NormalLogScore` to machine precision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}